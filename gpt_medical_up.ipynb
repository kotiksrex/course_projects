{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Попробовала дообучить rubert-tiny2 на датасете blinoff/medical_qa_ru_data.\n",
        "Обучать на CPU это боль)) в колабе не смогла дождаться, обучение прервалось через 6 часов. Сократила до 2-х эпох и урезала датасет. Доработала функцию предобработки текста. Лематизацию пришлось закомментрировать ввиду длительности процесса.   использовала импорт модели с модулем Взяла всего 20%, передала на cuda, дообучение модели занимает порядка 40-50 минут.  Дообученная модель выдает слишком короткие ответы.\n",
        "\n",
        "Модель cointegrated/rubert-tiny2 является русскоязычной моделью языковой модели BERT, обученной на небольшом размере данных. Вот подробная характеристика этой модели:\n",
        "\n",
        "- Название модели: cointegrated/rubert-tiny2\n",
        "- Базовая модель: RuBERT (Russian BERT)\n",
        "- Размер модели: Текущий размер модели составляет примерно 42,7 МБ.\n",
        "- Конфигурация модели: Это модель BERT, предназначенная для генерации условных последовательностей. Она может использоваться для задач, таких как текстовая генерация, ответы на вопросы, перевод и другие задачи, где требуется генерировать последовательности текста на основе окружающего контекста.\n",
        "\n",
        "Для дообучения модели для вопросно-ответной системы для чат-бота используется модуль AutoModelForCausalLM, который является частью библиотеки transformers и предоставляет предобученные модели для задачи генерации условных последовательностей (Causal Language Modeling).\n",
        "\n",
        "AutoModelForCausalLM:\n",
        "\n",
        "1. Модуль AutoModelForCausalLM предоставляет общий интерфейс для работы с различными моделями генерации условных последовательностей. Он может автоматически выбирать соответствующую модель, основываясь на названии модели, указанном в качестве параметра.\n",
        "\n",
        "2. Обеспечивает поддержку различных архитектур: может использоваться с различными предобученными архитектурами моделей, такими как BERT, GPT, GPT-2, XLNet и другими, что позволяет выбрать подходящую модель для конкретной задачи генерации текста.\n",
        "\n",
        "3. Использование предобученных весов: Модуль AutoModelForCausalLM загружает предобученные веса модели, которые позволяют использовать модель для генерации текста с уже полученными знаниями о языке."
      ],
      "metadata": {
        "id": "xtPBqLGJns_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_iT26DzbQaoW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f3fbec-6569-4e50-fe94-121a3c638a25",
        "id": "yGKVEHh3ebqu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuXkW8ObBw9K",
        "outputId": "79e8a1f7-852b-4818-e62b-157b77395476"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6se3ZDHb75B",
        "outputId": "5bfb3a31-a0cc-4e33-d714-cd0a661ea9fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufbgunV3k3YD",
        "outputId": "5425c27b-0748-4a4e-8a4b-271039ee72d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-21 04:42:26--  https://huggingface.co/datasets/blinoff/medical_qa_ru_data/resolve/main/medical_qa_ru_data.csv\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.27, 65.8.178.118, 65.8.178.93, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/datasets/blinoff/medical_qa_ru_data/6aca4fb6dae499745c9ca1dfad8a0ade809573daf228c923fdf846ba4a639b50?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27medical_qa_ru_data.csv%3B+filename%3D%22medical_qa_ru_data.csv%22%3B&response-content-type=text%2Fcsv&Expires=1695530547&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NTUzMDU0N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9ibGlub2ZmL21lZGljYWxfcWFfcnVfZGF0YS82YWNhNGZiNmRhZTQ5OTc0NWM5Y2ExZGZhZDhhMGFkZTgwOTU3M2RhZjIyOGM5MjNmZGY4NDZiYTRhNjM5YjUwP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=hJmGTSDQpjCLurb4iD%7EHUy8qXpYD18oxN2Xi8mGoBwxWciadaTDjgw14iFNFs897CiPlVJ9-eeAowU0DX%7Eyb0Mol%7EQPBxKCYdDClsNgQVKSSyAcWGUBF-v5fMTPjQYsEIrFyjDRqSGSe3uQ5VktbvOQWqAeEwrf36y34qVIxlfgg6EgvtNOVp4KBUS8UBHLNN-7n1Vj13Db5b0bNonmZAs-cF-LfubM3Rjr%7ECRJ9Nu-L6Z77-1tp2sv7agda8V8Qjquej6YYjK13hFObKNJ8d9m82B5viThmR29Bg9dxcXgmPI8ciRVaSuCWPmSFdJDR2NCtgJENj-syMO7VzbHkMw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-09-21 04:42:27--  https://cdn-lfs.huggingface.co/datasets/blinoff/medical_qa_ru_data/6aca4fb6dae499745c9ca1dfad8a0ade809573daf228c923fdf846ba4a639b50?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27medical_qa_ru_data.csv%3B+filename%3D%22medical_qa_ru_data.csv%22%3B&response-content-type=text%2Fcsv&Expires=1695530547&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NTUzMDU0N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9ibGlub2ZmL21lZGljYWxfcWFfcnVfZGF0YS82YWNhNGZiNmRhZTQ5OTc0NWM5Y2ExZGZhZDhhMGFkZTgwOTU3M2RhZjIyOGM5MjNmZGY4NDZiYTRhNjM5YjUwP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=hJmGTSDQpjCLurb4iD%7EHUy8qXpYD18oxN2Xi8mGoBwxWciadaTDjgw14iFNFs897CiPlVJ9-eeAowU0DX%7Eyb0Mol%7EQPBxKCYdDClsNgQVKSSyAcWGUBF-v5fMTPjQYsEIrFyjDRqSGSe3uQ5VktbvOQWqAeEwrf36y34qVIxlfgg6EgvtNOVp4KBUS8UBHLNN-7n1Vj13Db5b0bNonmZAs-cF-LfubM3Rjr%7ECRJ9Nu-L6Z77-1tp2sv7agda8V8Qjquej6YYjK13hFObKNJ8d9m82B5viThmR29Bg9dxcXgmPI8ciRVaSuCWPmSFdJDR2NCtgJENj-syMO7VzbHkMw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.162.58, 108.157.162.99, 108.157.162.27, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.162.58|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 265838965 (254M) [text/csv]\n",
            "Saving to: ‘medical_qa_ru_data.csv.1’\n",
            "\n",
            "medical_qa_ru_data. 100%[===================>] 253.52M  47.5MB/s    in 5.5s    \n",
            "\n",
            "2023-09-21 04:42:33 (45.8 MB/s) - ‘medical_qa_ru_data.csv.1’ saved [265838965/265838965]\n",
            "\n",
            "Requirement already satisfied: corus in /usr/local/lib/python3.10/dist-packages (0.10.0)\n"
          ]
        }
      ],
      "source": [
        "! wget https://huggingface.co/datasets/blinoff/medical_qa_ru_data/resolve/main/medical_qa_ru_data.csv\n",
        "\n",
        "! pip install corus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dbrxr3ccAfa",
        "outputId": "c17b0621-ba5f-4132-b1ed-c0f88721a779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: transformers==4.28.0 in /usr/local/lib/python3.10/dist-packages (4.28.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers==4.28.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "!pip install re\n",
        "!pip install nltk\n",
        "!pip install unicodedata\n",
        "!pip install contractions\n",
        "!pip install inflect\n",
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StVCNIh-P2bd",
        "outputId": "e216f3a7-944a-4e90-9487-24bc97e00839"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.10/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.10/dist-packages (from pymorphy2) (0.6.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement unicodedata (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for unicodedata\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (7.0.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect) (1.10.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from inflect) (4.5.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9DKk-JKYwDn",
        "outputId": "613d5040-d673-4538-f855-1eb7573d330f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.12)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_m =  \"/content/drive/MyDrive/Data Science_models/nlp/project/\""
      ],
      "metadata": {
        "id": "h_zD-W4RNp4u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "#from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import pymorphy2\n",
        "import re\n",
        "\n",
        "import emoji\n",
        "import unicodedata\n",
        "import contractions\n",
        "import inflect\n",
        "from num2words import num2words\n"
      ],
      "metadata": {
        "id": "7hLyhA-2RjjI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')  # Токенизатор\n",
        "nltk.download('stopwords')  # Стоп-слова\n",
        "nltk.download('wordnet')  # Лемматизатор"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HSQjAGjRnQH",
        "outputId": "3d49752c-3037-4ac3-f98e-074a8b50d61e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "#from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "#from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "QlxLobLsf7DQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка набора данных Medical QA Ru Data\n",
        "dataset = load_dataset('blinoff/medical_qa_ru_data', split= 'train[:10%]')\n",
        "\n"
      ],
      "metadata": {
        "id": "EKcTZZ6-Dg9y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование Dataset в список словарей\n",
        "dataset_dict = dataset.to_dict()\n",
        "\n",
        "# Преобразование списка словарей в объект Pandas DataFrame\n",
        "data = pd.DataFrame(dataset_dict, columns = ['desc', 'ans'])\n",
        "\n",
        "# Просмотр первых 5 строк датафрейма\n",
        "print(data.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lAlYvT8iPME",
        "outputId": "14839e68-4bbf-4d6f-aa65-65cad7adb0f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                desc  \\\n",
            "0  Ларипронт 20 талеток,через каждые 2-3 часа.Оче...   \n",
            "1  Здравствуйте, я на 7-8 неделе беременности. С ...   \n",
            "2  Здравствуйте месячные должны придти 23 марта в...   \n",
            "3  Завтра иду с утра сдавать кровь ТТГ, Т4СВ, Кал...   \n",
            "4  Мне прописали пить Аллохол. Врач написала пить...   \n",
            "\n",
            "                                                 ans  \n",
            "0  Что вы им лечите? Длительность приема Ларипрон...  \n",
            "1  Здравствуйте, это может быть признаком раннего...  \n",
            "2                         Выполните исследование хгч  \n",
            "3  Можно.;\\nЗдравствуйте , да, попейте сладкого ч...  \n",
            "4  Препарат принимается после еды. Уточните это  ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['desc'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "XG3-rgPLyaFA",
        "outputId": "57d70c65-a4d8-4f09-93ac-0079a7fb3bad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ларипронт 20 талеток,через каждые 2-3 часа.Очень понравились,но пока принимала,чувствовала себя хорошо. Закончились-все сиптомы опять проявились. Нигде не найду как долго их можно принимать. Скажите пожалуйста.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['ans'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rgHdd9Hg0IUX",
        "outputId": "afc7878c-cd1f-450c-974b-8ee6297e2c0e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Что вы им лечите? Длительность приема Ларипронта индивидуальна и устанавливается лечащим врачом.;\\nУточните у своего лечащего врача.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['spec10'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_qZHlucI1pMP",
        "outputId": "af700450-2cc7-40bc-c140-00ca7e49468a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Отоларинголог'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KJW4lV0675dC"
      },
      "outputs": [],
      "source": [
        "train_q, test_q, train_ans, test_ans = train_test_split(data.desc,\n",
        "                                                        data.ans,\n",
        "                                                        test_size=.2,\n",
        "                                                        random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SH2e3RMdmQ4R"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модуль AutoModelForCausalLM и модуль AutoModelForQuestionAnswering являются двумя разными модулями, предназначенными для различных задач.\n",
        "\n",
        "Модуль AutoModelForCausalLM предназначен для создания автоматически настраиваемой модели для генерации последовательностей, где последовательности имеют причинно-следственную связь. Например, такая модель может использоваться для генерации продолжения рассказа или прогнозирования следующего слова в текстовой последовательности.\n",
        "\n",
        "С другой стороны, модуль AutoModelForQuestionAnswering предназначен для создания автоматически настраиваемой модели для ответа на вопросы на основе контекстных данных. Например, такая модель может использоваться для отвечения на вопросы в рамках задачи вопросно-ответной системы, в которой нужно найти ответ на вопрос в текстовом документе или корпусе данных."
      ],
      "metadata": {
        "id": "VUTFdy8KcHII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model_name = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "#tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
        "#model = AutoModelForCausalLM.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2').to(device)"
      ],
      "metadata": {
        "id": "F2J-d4JiAu9V"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Инициализация токенизатора и модели BERT\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"cointegrated/rubert-tiny2\").to(device)\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(\"cointegrated/rubert-tiny2\").to(device)\n",
        "#model.cuda()  # uncomment it if you have a GPU\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "TCcn3UT1Z--q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmobqWOUxc9X",
        "outputId": "f37163d7-d9fc-40bd-c863-1d0e6e836300"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2-4Jeae5O_Rx"
      },
      "outputs": [],
      "source": [
        "# Функция для очистки текста из статьи https://habr.com/ru/articles/738176/ адаптированная под русский язык\n",
        "def preproc_text(text):\n",
        "    \"\"\"Очищает текст от HTML-тегов, URL-ссылок, эмоджи, приводит к нижнему регистру,\n",
        "    удаляет специальные символы, удаляет стоп-слова,\n",
        "    лемматизирует слова и удаляет знаки препинания.\n",
        "\n",
        "    Args:\n",
        "        input_text (str): Входной текст для очистки.\n",
        "\n",
        "    Returns:\n",
        "        str: Очищенный текст.\n",
        "    \"\"\"\n",
        "\n",
        "    res = str(text).strip()\n",
        "\n",
        "    # HTML-теги: первый шаг - удалить из входного текста все HTML-теги\n",
        "    res = re.sub('<[^<]+?>', '', res)\n",
        "\n",
        "    # URL и ссылки: далее - удаляем из текста все URL и ссылки\n",
        "    res = re.sub(r'http\\S+', '', res)\n",
        "\n",
        "    # Эмоджи и эмотиконы: используем собственную функцию для преобразования эмоджи в текст\n",
        "    res = emojis_words(res)\n",
        "\n",
        "    # Приводим все входные данные к нижнему регистру\n",
        "    res = res.lower()\n",
        "\n",
        "    # Убираем лишние пробелы\n",
        "    res = re.sub(r\"\\s+\", \" \", res)\n",
        "\n",
        "    # Убираем специальные символы: избавляемся от всего, что не является \"словами\"\n",
        "    res = re.sub('[^а-яА-ЯёЁ0-9\\s]', '', res)\n",
        "\n",
        "    # Стоп-слова: удаление стоп-слов - это стандартная практика очистки текстов\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "    words = res.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    res = ' '.join(words)\n",
        "\n",
        "    # Лемматизация слов\n",
        "    #morph = pymorphy2.MorphAnalyzer()\n",
        "   # lemmas = [morph.parse(word)[0].normal_form for word in words] # этот процесс сильно замедляет обработку текста (3 часа на 20% данного датасета)\n",
        "    #res = ' '.join(lemmas)\n",
        "    # Знаки препинания: далее - удаляем из текста все знаки препинания\n",
        "    res = re.sub(r'[^\\w\\s]', '', res)\n",
        "\n",
        "    return res\n",
        "\n",
        "# Функция для преобразования эмоджи в слова\n",
        "def emojis_words(text):\n",
        "\n",
        "    # Модуль emoji: преобразование эмоджи в их словесные описания\n",
        "    clean_text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "\n",
        "    # Редактирование текста путём замены \":\" и\" _\", а так же - путём добавления пробела между отдельными словами\n",
        "    clean_text = clean_text.replace(\":\", \"\").replace(\"_\", \" \")\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def build_data(data_q, data_ans):\n",
        "    data = []\n",
        "    for idx, texts in enumerate(data_q):\n",
        "        question = preproc_text(texts)\n",
        "        answer = preproc_text(data_ans.iloc[idx])\n",
        "        res = '\\nx:' + question + '\\ny:' + answer\n",
        "        data.append(res)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data = build_data(train_q, train_ans)\n",
        "#test_data = build_data(test_q, test_ans)"
      ],
      "metadata": {
        "id": "sKz95ObEr0x8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DK43-7W5QzPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88af0e4b-40e0-4133-fef9-e3e328bd5b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train data: 100%|██████████| 15227/15227 [00:31<00:00, 488.71it/s]\n",
            "Processing test data: 100%|██████████| 3807/3807 [00:04<00:00, 784.35it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Форматирование тренировочных данных с прогресс-баром\n",
        "train_data = []\n",
        "for idx, texts in tqdm(enumerate(train_q), total=len(train_q), desc='Processing train data'):\n",
        "    question = preproc_text(texts)\n",
        "    answer = preproc_text(train_ans.iloc[idx])\n",
        "    res = '\\nx:' + question + '\\ny:' + answer\n",
        "    train_data.append(res)\n",
        "\n",
        "# Форматирование тестовых данных с прогресс-баром\n",
        "test_data = []\n",
        "for idx, texts in tqdm(enumerate(test_q), total=len(test_q), desc='Processing test data'):\n",
        "    question = preproc_text(texts)\n",
        "    answer = preproc_text(test_ans.iloc[idx])\n",
        "    res = '\\nx:' + question + '\\ny:' + answer\n",
        "    test_data.append(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Этот код устанавливает токен, который будет использоваться при заполнении (padding) последовательностей текстовых данных при обучении нейронных сетей. Это делается с помощью установки значения eos_token (токена конца последовательности) как значения pad_token (токена-заполнителя).\n",
        "\n",
        "Обычно в обучающем наборе данных бывают последовательности, которые имеют разную длину. Для обеспечения удобства и эффективности обучения модели, все последовательности должны быть одинаковой длины. Один из подходов к достижению этой цели - добавление токенов-заполнителей в конец последовательности до тех пор, пока ее длина не станет равной максимальной длине в обучающем наборе данных.\n",
        "\n",
        "Установка pad_token в значение eos_token означает, что при заполнении последовательности токеном-заполнителем будет использоваться токен, обозначающий конец последовательности. Это может быть полезно, когда необходимо предотвратить модель от генерации ненужной информации после конца последовательности, ведь такие лишние токены могут негативно сказаться на точности модели."
      ],
      "metadata": {
        "id": "X-NK9FzxdNLW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AYDjvuqTJppJ"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ],
      "metadata": {
        "id": "fBQnRk0_ogjT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1BxCf_FdAqJ"
      },
      "source": [
        "Токенизируем тренировочный и тестовый датасеты:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "znS_qmnERL-w"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_data, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "test_encodings = tokenizer(test_data, padding='max_length', truncation=True, max_length=512, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CE6tpq2-Rfrs"
      },
      "outputs": [],
      "source": [
        "class QAMedicalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "train_dataset = QAMedicalDataset(train_encodings)\n",
        "test_dataset = QAMedicalDataset(test_encodings)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка тренировочных аргументов\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./models\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved\n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    )\n",
        "\n",
        "# Задание значения max_steps перед инициализацией тренера\n",
        "#training_args.max_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
        "\n",
        "# Определение тренера\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euYKwmEmxedT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение модели\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DBDfRLkFxn9D",
        "outputId": "3d249ff5-b1f9-4c9f-9d29-6556a5c10885"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11421' max='11421' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11421/11421 1:56:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.847900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.508200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>4.365600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.330700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.249300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.198700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>4.189200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.031000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>3.874600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>3.867100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>3.849000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>3.840400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>3.834800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>3.828500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>3.842800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>3.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>3.577800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>3.608700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>3.575400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>3.598800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>3.585800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>3.592900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-28-b29274360c54>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=11421, training_loss=3.9335840557723984, metrics={'train_runtime': 6968.6398, 'train_samples_per_second': 6.555, 'train_steps_per_second': 1.639, 'total_flos': 1.1936081313792e+16, 'train_loss': 3.9335840557723984, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXjLIgB7dt2Y"
      },
      "source": [
        "Сохраним модель на диск:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(path_m + 'tokenizer_qa_medical_up_3')\n",
        "#model.save_pretrained(path_m + 'model_qa_medical_up_1')\n",
        "model.save_pretrained(path_m + 'model_qa_medical_up_3')"
      ],
      "metadata": {
        "id": "KoJK8wh0N-tK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cLxhxA9LfjMh"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(path_m + 'tokenizer_qa_medical_up_3')\n",
        "model = AutoModelForCausalLM.from_pretrained(path_m + 'model_qa_medical_up_3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-jSsKcfd0xD"
      },
      "source": [
        "Функция для ведения диалога. Модель \"держит в голове\" контекст последних 10-ти сообщений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TZ6KwIZ9rX5Q"
      },
      "outputs": [],
      "source": [
        "def respond_to_dialog(texts):\n",
        "    prefix = '\\nx:'\n",
        "    for i, t in enumerate(texts):\n",
        "        prefix += t\n",
        "        prefix += '\\nx:' if i % 2 == 1 else '\\ny:'\n",
        "    tokens = tokenizer(prefix, return_tensors='pt')\n",
        "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
        "    end_token_id = tokenizer.encode('\\n')[0]\n",
        "    size = tokens['input_ids'].shape[1]\n",
        "    output = model.generate(\n",
        "        **tokens,\n",
        "        eos_token_id=end_token_id,\n",
        "        do_sample=True,\n",
        "        max_length=size+128,\n",
        "        repetition_penalty=3.2,\n",
        "        temperature=1,\n",
        "        num_beams=3,\n",
        "        length_penalty=0.01,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    decoded = tokenizer.decode(output[0])\n",
        "    result = re.findall(r'\\ny:(.+)', decoded)[-1]\n",
        "    return result.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMVl7jePeBYu"
      },
      "source": [
        "Протестируем генератор:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z6qrX4JrjHP",
        "outputId": "6d72ae31-92f2-4056-dd8b-8d86bdec2380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Начните диалог с ботом любой фразой\n",
            "как лечить горло\n",
            "здравствуйте орошать мирамистином полоскать фурациллиномсмазывать хлоргексидиномпринимать антибиотик пользователь поблагодарил ответ сумму 10 бонусов текст благодарности спасибо консультацию можете орошать мирамистином полоскание горла раствором соды рассасывания лизобакта 1т3р рать 7дн ингалипт 2в3р горло закапывать борную кислоту 3 раза день температура тела должна повышаться выше 385 372 принимать жаропонижающие препараты обратитесь терапевту очном порядкене занимайтесь самолечением здравствуйте начните циклоферон парацетам\n",
            "что делать, если болит нога\n",
            "выполните снимок сустава покажитесь травматологу выполните рентген мрт суставапокажитесь ортопеду нужно исключить перелом голеностопного сустава возможно развитие артрита ревматоидного артритавам нужно выполнить рентгенографию сустава показаться травматологувозможен остеохондроз межпозвонкового дискаисключите патологию поясничнокрестцового отдела позвоночникадля назначения лечения обратитесь неврологу исключите шохпокажитесь неврологу сдайте кровь вэб цмв впг токсоплазму микоплазмы хламидии респираторные методом ифа\n",
            "болит голова на затылке\n",
            "нужно обратиться неврологу обследовать шейный отдел позвоночника проверить функцию щитовидной железы проверьте уровень гормонов щитовидной железы узи органов брюшной полости консультация невролога лечение интернету переписке назначается обязательно посетите невролога осмотр невролога обязателен нужен осмотр невролога возможен миозит межпозвоночных дискованевризма грыжа диска протрузии проконсультируйтесь нейрохирургом необходим осмотр нейрохирурга это лишь вспомогательный метод диагностики уточнения диагноза проведения консервативного лечения массаж воротниковой зоны физиопроцедуры электрофорез контрастированием гигромедиапроцеду\n",
            "живот болит\n",
            "скорее имеет место остеохондроз межпозвоночных дисков необходимо выполнить рентгенографию грудного отдела позвоночника контроль ад пульса артериального давления уздг сосудов головы шеи общий анализ крови мочи биохимический анализ крови эозинофилы мазки зева флору влагалища соскоб пцр группы инфекций методом пцр инфекции микрофлору чувствительностью антибиотикам посев чувствительность антибиотикам определение чувствительности антибиотикам иммуноглобулинов исследованием секрета предстательной железы урогенитального тракта уреалитикумбилипендиметазаочноне видя суть проблемы невозможно поставить\n",
            "Стоп\n"
          ]
        }
      ],
      "source": [
        "seed = input('Начните диалог с ботом любой фразой\\n')\n",
        "history = [seed]\n",
        "while True:\n",
        "    result = respond_to_dialog(history[-10:])\n",
        "    next_sentence = input(result + '\\n')\n",
        "    if next_sentence == 'Стоп':\n",
        "        break\n",
        "    history.append(result)\n",
        "    history.append(next_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат далек от совершенства, но лучше, чем с минимальной предобработкой текста. Явный недостаток- модель не разделяет некоторые слова."
      ],
      "metadata": {
        "id": "4YGy_5qyiIRk"
      }
    }
  ]
}